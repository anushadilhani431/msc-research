{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc717cd4-1e18-4b50-984b-fadc54799f6b",
   "metadata": {},
   "source": [
    "## Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "212460b6-01d5-40e9-9df2-38c355006ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta, timezone\n",
    "import time\n",
    "from pathlib import Path\n",
    "import MetaTrader5 as mt5\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70799de4-3028-4c98-b493-eba568ef8f1f",
   "metadata": {},
   "source": [
    "## Configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f086c9-4a54-4e1f-af92-d0d3ff36a8a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "SYMBOL = \"EURUSD\"\n",
    "TIMEFRAME = mt5.TIMEFRAME_M5          # 5-minute candles\n",
    "DAYS_PER_CHUNK = 180                  # ~6 months per request; adjust if needed\n",
    "MAX_ITERS = 1000                      # safety stop\n",
    "OUT_DIR = Path(\"raw_data\")   # each year -> one CSV file\n",
    "GROUP_BY_TZ = \"UTC\"                   # \"UTC\" or e.g., \"Asia/Colombo\"\n",
    "PAUSE_SEC = 0.2                       # brief pause to let MT5 fetch missing history\n",
    "MAX_EMPTY_ADVANCES = 365              # safety guard when scanning forward\n",
    "\n",
    "LOGIN = None       \n",
    "SERVER = None     \n",
    "PASSWORD = None   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc75765-71a2-4b60-b39c-029d086f5806",
   "metadata": {},
   "source": [
    "## Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "98379994-256a-4300-96f8-957cfa0df241",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mt5():\n",
    "    if not mt5.initialize():\n",
    "        raise RuntimeError(f\"MT5 initialize() failed: {mt5.last_error()}\")\n",
    "\n",
    "    if LOGIN and SERVER and PASSWORD:\n",
    "        if not mt5.login(LOGIN, password=PASSWORD, server=SERVER):\n",
    "            err = mt5.last_error()\n",
    "            mt5.shutdown()\n",
    "            raise RuntimeError(f\"Login failed: {err}\")\n",
    "\n",
    "    if not mt5.symbol_select(SYMBOL, True):\n",
    "        mt5.shutdown()\n",
    "        raise RuntimeError(f\"Failed to select symbol: {SYMBOL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adc51be9-bb02-4880-9415-abe728d80366",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_chunk_by_year(df: pd.DataFrame, out_dir: Path, group_by_tz: str = \"UTC\"):\n",
    "    \"\"\"\n",
    "    Split given DataFrame (with UTC-aware 'time') into yearly CSVs.\n",
    "    - Files are named: {SYMBOL}_M5_{YYYY}.csv\n",
    "    - Data is written in chronological order per year.\n",
    "    - We keep 'time' in UTC for fidelity; year grouping can use another tz if desired.\n",
    "    \"\"\"\n",
    "    if df.empty:\n",
    "        return\n",
    "\n",
    "    # Ensure columns & dtypes\n",
    "    expected_cols = [\"time\", \"open\", \"high\", \"low\", \"close\", \"tick_volume\", \"spread\", \"real_volume\"]\n",
    "    df = df.loc[:, expected_cols].drop_duplicates(subset=[\"time\"])\n",
    "    df = df.sort_values(\"time\")\n",
    "\n",
    "    # Determine year for grouping\n",
    "    if group_by_tz.upper() == \"UTC\":\n",
    "        years = df[\"time\"].dt.year\n",
    "    else:\n",
    "        # Convert to target tz only for grouping boundaries\n",
    "        years = df[\"time\"].dt.tz_convert(group_by_tz).dt.year\n",
    "\n",
    "    for year, part in df.groupby(years):\n",
    "        part = part.sort_values(\"time\")\n",
    "        out_path = out_dir / f\"{SYMBOL}_M5_{year}.csv\"\n",
    "        header = not out_path.exists()\n",
    "        part.to_csv(out_path, index=False, mode=\"a\", header=header)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23bca542-fbcd-4cf9-9529-b200bdd1990f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_bar_on_or_after(date_utc):\n",
    "    \"\"\"\n",
    "    Return the timestamp (UTC-aware pandas Timestamp) of the first available bar\n",
    "    whose open time is >= date_utc. Advances in weekly steps until it finds one.\n",
    "    \"\"\"\n",
    "    probe = date_utc\n",
    "    empties = 0\n",
    "    step = timedelta(days=7)\n",
    "\n",
    "    while empties < MAX_EMPTY_ADVANCES:\n",
    "        # copy_rates_from returns bars starting from date_utc going forward\n",
    "        r = mt5.copy_rates_from(SYMBOL, TIMEFRAME, probe, 1)\n",
    "        if r is None:\n",
    "            raise RuntimeError(f\"copy_rates_from failed: {mt5.last_error()}\")\n",
    "\n",
    "        if len(r) > 0:\n",
    "            first_ts = pd.to_datetime(r[0]['time'], unit='s', utc=True)\n",
    "            return first_ts\n",
    "\n",
    "        # No data yet; step forward a bit and try again\n",
    "        probe += step\n",
    "        empties += 1\n",
    "        time.sleep(PAUSE_SEC)\n",
    "\n",
    "    return None  # likely no history that far back with this broker/timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0211cc3-1281-48e1-8c28-d6af86cabeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_history_and_write():\n",
    "    \"\"\"\n",
    "    Pages FORWARD in time using copy_rates_range() starting from the first\n",
    "    trading bar on/after 2000-01-01 UTC, writing each chunk to per-year CSVs.\n",
    "    \"\"\"\n",
    "    OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if not mt5.initialize():\n",
    "        raise SystemExit(f\"initialize() failed: {mt5.last_error()}\")\n",
    "\n",
    "    # 1) Find the first available bar on/after 2000-01-01 (UTC)\n",
    "    start_hint = datetime(2000, 1, 1, tzinfo=timezone.utc)\n",
    "    first_bar_ts = find_first_bar_on_or_after(start_hint)\n",
    "    if first_bar_ts is None:\n",
    "        mt5.shutdown()\n",
    "        print(\"No history found after 2000-01-01 for this symbol/timeframe.\")\n",
    "        return\n",
    "\n",
    "    # 2) Forward-page in chunks until 'now'\n",
    "    chunk = timedelta(days=DAYS_PER_CHUNK)\n",
    "    end_of_range = datetime.now(timezone.utc)\n",
    "    total_rows = 0\n",
    "    windows = 0\n",
    "    cur_start = first_bar_ts.to_pydatetime()\n",
    "\n",
    "    while cur_start < end_of_range:\n",
    "        cur_end = min(cur_start + chunk, end_of_range)\n",
    "\n",
    "        rates = mt5.copy_rates_range(SYMBOL, TIMEFRAME, cur_start, cur_end)\n",
    "        if rates is None:\n",
    "            print(\"copy_rates_range returned None; error:\", mt5.last_error())\n",
    "            break\n",
    "\n",
    "        if len(rates) == 0:\n",
    "            # No bars in this window (e.g., holidays; far in the past)\n",
    "            # Move forward and continue scanning.\n",
    "            cur_start = cur_end + timedelta(seconds=1)\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(rates)\n",
    "        df[\"time\"] = pd.to_datetime(df[\"time\"], unit=\"s\", utc=True)\n",
    "\n",
    "        # Write to year-split files; you said this handles duplicates internally\n",
    "        write_chunk_by_year(df, OUT_DIR, GROUP_BY_TZ)\n",
    "\n",
    "        total_rows += len(df)\n",
    "        windows += 1\n",
    "\n",
    "        # Advance start to just after the last bar we actually received\n",
    "        last_ts = df[\"time\"].iat[-1].to_pydatetime()\n",
    "        cur_start = last_ts + timedelta(seconds=1)\n",
    "\n",
    "        if windows % 5 == 0:\n",
    "            print(f\"Fetched {windows} windows; total rows so far: {total_rows}\")\n",
    "\n",
    "        time.sleep(PAUSE_SEC)\n",
    "\n",
    "    mt5.shutdown()\n",
    "    print(f\"Done. Wrote data to: {OUT_DIR.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd944348-1a74-48e8-b719-006582f0815f",
   "metadata": {},
   "source": [
    "## Download data using helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b16971f8-a1bf-45f2-a6fa-27be2908a6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched 5 windows; total rows so far: 182286\n",
      "Fetched 10 windows; total rows so far: 366174\n",
      "Fetched 15 windows; total rows so far: 549534\n",
      "Fetched 20 windows; total rows so far: 729941\n",
      "Fetched 25 windows; total rows so far: 912178\n",
      "Fetched 30 windows; total rows so far: 1093614\n",
      "Fetched 35 windows; total rows so far: 1276224\n",
      "Fetched 40 windows; total rows so far: 1459858\n",
      "Fetched 45 windows; total rows so far: 1643214\n"
     ]
    }
   ],
   "source": [
    "init_mt5()\n",
    "try:\n",
    "    fetch_history_and_write()\n",
    "finally:\n",
    "    mt5.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7d0268-1818-455f-9a11-0332cee18861",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (trading-env)",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
